# Adv Pentaho Data Integration - DI 1100
This course is aimed at introducing the Pentaho ETL tool to folks who are involved in a Pentaho implementation. Data Analysts, Project Managers, Professional Services consultants, Data Architects, Report Designers will gain a solid understanding of the key ETL concepts and workflows.

## Getting Started
These instructions will get you up and running on your local machine for development and testing purposes. 

### Prerequisites
The following software need to be installed and configured:

```
Pentaho Business Analytics 8.x
Java JDK 9.0.x
Docker for Windows
Git
Jenkins
```

### Installing
A step by step series of guides can be found at:


```
* [Dropwizard](http://www.dropwizard.io/1.0.2/docs/) - The web framework used
* [Maven](https://maven.apache.org/) - Dependency Management
* [ROME](https://rometools.github.io/rome/) - Used to generate RSS Feeds
```

## Course Overview
On completing this course, you will be able to:

### Module 1 - Project / Lifecycle Management
```
 Deploy PDI projects for: 
  * Development - local file repository
  * UAT - EE repository 
  * Production - EE repository
```

### Module 2 - PDI as a Data Source
```
  Configure PDI as a datasource for various scenarios:
  * Pentaho Reports step
  * CDA
  * Machine Learning
  * Data Services
```  
### Module 3 - Streaming Data
```
  Implement a MQTT Broker 
  * Stream GPS co-ordinates to PDI to demonstrate IoT
  Implement Kafka
  * Twitter Stream - you will need a twitter account
```
## Versioning
We use [SemVer](http://semver.org/) for versioning. For the versions available, see the [tags on this repository](https://github.com/your/project/tags). 


## Acknowledgments
```
* Hat tip to anyone who's code was used
* Inspiration
* etc
=======
```
