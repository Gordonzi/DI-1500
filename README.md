# Advanced Pentaho Data Integration - DI 1500
This course is aimed at introducing the Pentaho ETL tool to folks who are involved in a Pentaho implementation. Data Analysts, Project Managers, Professional Services consultants, Data Architects, Report Designers will gain a solid understanding of the key ETL concepts and workflows.

## Getting Started
These instructions will get you up and running on your local machine for development and testing purposes.

### Prerequisites
The following software need to be installed and configured:
```
Pentaho Business Analytics 8.x
Java JDK 9.0.x
Docker for Windows
Git
Visual Code Source
R
RStudio
Jenkins
```

### Installing
A step by step series of guides can be found at:

[Artefacts for DI 5000](https://www.dropbox.com/sh/6nl31ts10sjimnr/AADFXjTek4f9ANyBivVVAhqFa?dl=0) - Shared File on DropBox



## Course Overview
On completing this course, you will be able to:

### Module 1 - Project / Lifecycle Management
```
 Deploy PDI projects for:
  * Local - local file repository (with EE option)
  * Development - EE Repository
  * UAT - EE repository
  * Production - EE repository
```

### Module 2 - PDI as a Data Source
```
  Configure PDI as a datasource for various scenarios:
  * Pentaho Reports step
  * CDA
  * Machine Learning
  * Data Services
```  

### Module 3 - Streaming Data
```
  Implement a MQTT Broker
  * Stream GPS co-ordinates to PDI to demonstrate IoT
  Implement Kafka
  * Twitter Stream - you will need a twitter account
```

### Module 4 - Metadata Injection
```
  Overview of Metadata Injection
  * Metadata Injection Workflows
  * Metadata Injection Scenarios
```

### Module 5 - Scalability
```
  Master & Slave nodes
  Clustering
  Worker Nodes * this is currently under development
  Partitioning
  Scheduling
  Checkpoints

```

## Acknowledgments
```
Beppe Raymaekers
Morgan Senechal
Caio Moreno de Souza
```
